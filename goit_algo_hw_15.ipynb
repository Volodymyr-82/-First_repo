{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNwKjk4JY6z9mtgQoEU2IG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volodymyr-82/-First_repo/blob/main/goit_algo_hw_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v_Yn2ogQ_qbJ"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "Jy54qhzo_xN9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "9OMJIKYo_2x2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Текст для обробки\n",
        "import nltk\n",
        "\n",
        "# завантажуємо потрібні ресурси\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")   # новий ресурс, який з'явився у NLTK 3.9+\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"This is a test. Let's see how tokenization works!\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "print(\"Токени:\", tokens)\n",
        "print(\"Речення:\", sentences)\n",
        "print(\"Стоп-слова:\", list(stop_words)[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwVIr-Gb_9ww",
        "outputId": "14b0ab73-967a-4973-8fdf-a5c8a04e5d23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токени: ['This', 'is', 'a', 'test', '.', 'Let', \"'s\", 'see', 'how', 'tokenization', 'works', '!']\n",
            "Речення: ['This is a test.', \"Let's see how tokenization works!\"]\n",
            "Стоп-слова: ['be', \"couldn't\", \"you'll\", 'where', \"don't\", 'few', 'each', 'him', 'can', \"he'll\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "6sFJg4tiAF6G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# базовий набір пунктуації\n",
        "punctuation = string.punctuation\n",
        "\n",
        "# додаємо перевід рядка\n",
        "punctuation = punctuation + '\\n'\n",
        "\n",
        "print(punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT9q4P2cAQ3u",
        "outputId": "2222b6a1-59b8-4c75-e824-275b731ecd36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# беремо англійські стоп-слова\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# набір пунктуації\n",
        "punctuation = string.punctuation + \"\\n\"\n",
        "\n",
        "word_frequencies = {}\n",
        "\n",
        "for word in doc:\n",
        "    w = word.text.lower()\n",
        "    if w not in stop_words and w not in punctuation:\n",
        "        if w not in word_frequencies:\n",
        "            word_frequencies[w] = 1\n",
        "        else:\n",
        "            word_frequencies[w] += 1\n",
        "\n",
        "print(word_frequencies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxZaFNQ7A79c",
        "outputId": "82b9f840-ff5e-4cfc-d392-00d0f8614ba9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'example': 1, 'sentence': 1, 'tokenization': 1, 'lemmatization': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest\n"
      ],
      "metadata": {
        "id": "zo9DfBd_BIK0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "\n",
        "# приклад тексту\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It allows computers to understand, interpret and generate human language.\n",
        "Summarization is one of the key applications of NLP.\n",
        "By extracting the most relevant sentences, we can generate a shorter version of the text.\"\"\"\n",
        "\n",
        "# 1. Токенізуємо текст на речення\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "# 2. Токенізуємо текст на слова\n",
        "word_tokens = word_tokenize(text.lower())\n",
        "\n",
        "# 3. Завантажуємо стоп-слова\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# 4. Створюємо частотний словник\n",
        "word_frequencies = {}\n",
        "for word in word_tokens:\n",
        "    if word not in stop_words and word not in punctuation:\n",
        "        word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "# 5. Оцінюємо кожне речення (сумуємо ваги слів)\n",
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies:\n",
        "            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word]\n",
        "\n",
        "# 6. Вибираємо кілька найважливіших речень\n",
        "select_length = max(1, int(len(sentence_tokens) * 0.3))  # наприклад 30%\n",
        "summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "print(\"ОРИГІНАЛ:\\n\", text)\n",
        "print(\"\\nРЕЗЮМЕ:\\n\", \" \".join(summary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1-8mN56BMAs",
        "outputId": "31808f37-b947-4e99-8fb0-da1ac2ac88c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ОРИГІНАЛ:\n",
            " Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
            "It allows computers to understand, interpret and generate human language.\n",
            "Summarization is one of the key applications of NLP.\n",
            "By extracting the most relevant sentences, we can generate a shorter version of the text.\n",
            "\n",
            "РЕЗЮМЕ:\n",
            " Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9oE5SokBbPc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}